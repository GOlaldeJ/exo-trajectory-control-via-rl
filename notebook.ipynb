{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "\n",
    "# Initialize PyBullet\n",
    "physicsClient = p.connect(p.GUI)\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "p.setGravity(0, 0, -9.81)\n",
    "\n",
    "# Load ground plane\n",
    "planeId = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# Load exoskeleton\n",
    "startPos = [0, 0, 1.215]  # Start position above ground\n",
    "startOrientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "robotId = p.loadURDF(\"urdf/Exoskeleton.urdf\", startPos, startOrientation)\n",
    "\n",
    "# Get joint information\n",
    "numJoints = p.getNumJoints(robotId)\n",
    "jointInfoList = []\n",
    "for i in range(numJoints):\n",
    "    jointInfo = p.getJointInfo(robotId, i)\n",
    "    jointInfoList.append(jointInfo)\n",
    "    print(f\"Joint {i}: {jointInfo[1].decode('utf-8')}\")\n",
    "\n",
    "# Set initial joint positions for standing\n",
    "jointPositions = {\n",
    "    'right_hip_yaw_joint': -0.15,\n",
    "    'right_hip_roll_joint': 0.0,\n",
    "    'right_hip_pitch_joint': -0.025,\n",
    "    'right_knee_joint': 0.225,\n",
    "    'right_ankle_roll_joint': 0.0,\n",
    "    'right_ankle_pitch_joint': -0.2,\n",
    "    'left_hip_yaw_joint': 0.15,\n",
    "    'left_hip_roll_joint': 0.0,\n",
    "    'left_hip_pitch_joint': -0.025,\n",
    "    'left_knee_joint': 0.225,\n",
    "    'left_ankle_roll_joint': 0.0,\n",
    "    'left_ankle_pitch_joint': -0.2\n",
    "}\n",
    "\n",
    "# Apply initial joint positions\n",
    "for joint_name, position in jointPositions.items():\n",
    "    for i in range(numJoints):\n",
    "        if jointInfoList[i][1].decode('utf-8') == joint_name:\n",
    "            p.resetJointState(robotId, i, position)\n",
    "            p.setJointMotorControl2(\n",
    "                robotId, i, p.POSITION_CONTROL,\n",
    "                targetPosition=position,\n",
    "                force=200\n",
    "            )\n",
    "            break\n",
    "\n",
    "# Configure debug visualizer\n",
    "p.resetDebugVisualizerCamera(\n",
    "    cameraDistance=2,\n",
    "    cameraYaw=0,\n",
    "    cameraPitch=-30,\n",
    "    cameraTargetPosition=[0, 0, 1]\n",
    ")\n",
    "\n",
    "# Simulation loop\n",
    "try:\n",
    "    while True:\n",
    "        p.stepSimulation()\n",
    "        time.sleep(1./240.)\n",
    "except KeyboardInterrupt:\n",
    "    p.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4893bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "\n",
    "class ExoskeletonEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, render: bool = False, task: str = \"stand\"):\n",
    "        super(ExoskeletonEnv, self).__init__()\n",
    "        self.render_mode = render\n",
    "        self.task = task\n",
    "\n",
    "        # Connect to PyBullet in GUI or DIRECT mode\n",
    "        if self.render_mode:\n",
    "            self.physicsClient = p.connect(p.GUI)\n",
    "        else:\n",
    "            self.physicsClient = p.connect(p.DIRECT)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.81)\n",
    "        \n",
    "        # Load the ground plane and exoskeleton URDF\n",
    "        self.planeId = p.loadURDF(\"plane.urdf\")\n",
    "        self.startPos = [0, 0, 1.215]\n",
    "        self.startOrientation = p.getQuaternionFromEuler([0, 0, 0])\n",
    "        self.robotId = p.loadURDF(\"urdf/Exoskeleton.urdf\", self.startPos, self.startOrientation)\n",
    "        \n",
    "        # Define the controlled joints\n",
    "        self.joint_names = [\n",
    "            'right_hip_yaw_joint', 'right_hip_roll_joint', 'right_hip_pitch_joint', \n",
    "            'right_knee_joint', 'right_ankle_roll_joint', 'right_ankle_pitch_joint',\n",
    "            'left_hip_yaw_joint', 'left_hip_roll_joint', 'left_hip_pitch_joint', \n",
    "            'left_knee_joint', 'left_ankle_roll_joint', 'left_ankle_pitch_joint'\n",
    "        ]\n",
    "        self.joint_indices = {}\n",
    "        self.num_joints = p.getNumJoints(self.robotId)\n",
    "        for i in range(self.num_joints):\n",
    "            info = p.getJointInfo(self.robotId, i)\n",
    "            name = info[1].decode('utf-8')\n",
    "            if name in self.joint_names:\n",
    "                self.joint_indices[name] = i\n",
    "\n",
    "        # Default (standing) joint positions as offsets\n",
    "        self.default_joint_positions = {\n",
    "            'right_hip_yaw_joint': -0.15, \n",
    "            'right_hip_roll_joint': 0.0, \n",
    "            'right_hip_pitch_joint': -0.025,\n",
    "            'right_knee_joint': 0.225, \n",
    "            'right_ankle_roll_joint': 0.0, \n",
    "            'right_ankle_pitch_joint': -0.2,\n",
    "            'left_hip_yaw_joint': 0.15, \n",
    "            'left_hip_roll_joint': 0.0, \n",
    "            'left_hip_pitch_joint': -0.025,\n",
    "            'left_knee_joint': 0.225, \n",
    "            'left_ankle_roll_joint': 0.0, \n",
    "            'left_ankle_pitch_joint': -0.2\n",
    "        }\n",
    "        \n",
    "        # Action space: small deviations (offsets) for each joint.\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(len(self.joint_names),), dtype=np.float32\n",
    "        )\n",
    "        # Observation space: joint positions, velocities, base position (3) and base orientation (3 Euler angles).\n",
    "        # Total observation dimension: 12 joints * 2 + 6 + 6 = 36.\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(36,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize smoothed action\n",
    "        self.prev_action = np.zeros(len(self.joint_names), dtype=np.float32)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        smoothing_factor = 0.05  # Adjust for smoother transitions\n",
    "        self.prev_action = (1 - smoothing_factor) * self.prev_action + smoothing_factor * action\n",
    "        \n",
    "        # Compute target positions for each joint.\n",
    "        joint_indices = [self.joint_indices[name] for name in self.joint_names]\n",
    "        target_positions = []\n",
    "\n",
    "        for idx, name in enumerate(self.joint_names):\n",
    "            joint_index = self.joint_indices[name]\n",
    "            \n",
    "            # Get the physical joint limits from the URDF\n",
    "            joint_info = p.getJointInfo(self.robotId, joint_index)\n",
    "            joint_lower_limit = joint_info[8]  # Lower limit in radians\n",
    "            joint_upper_limit = joint_info[9]  # Upper limit in radians\n",
    "\n",
    "            # Map the action (in [-1, 1]) to the joint's physical range\n",
    "            scaled_position = joint_lower_limit + (self.prev_action[idx] + 1) / 2 * (joint_upper_limit - joint_lower_limit)\n",
    "            \n",
    "            # Append the target position for the joint\n",
    "            target_positions.append(scaled_position)\n",
    "        \n",
    "        # Apply control to all joints at once using setJointMotorControlArray.\n",
    "        p.setJointMotorControlArray(\n",
    "            self.robotId,\n",
    "            jointIndices=joint_indices,\n",
    "            controlMode=p.POSITION_CONTROL,\n",
    "            targetPositions=target_positions,\n",
    "            forces=[200] * len(joint_indices),\n",
    "        )\n",
    "        \n",
    "        p.stepSimulation()\n",
    "        time.sleep(1./240.)\n",
    "        self.elapsed_time += 1 / 240  # Assuming 240Hz simulation\n",
    "        \n",
    "        # Gather joint positions and velocities.\n",
    "        joint_positions = []\n",
    "        joint_velocities = []\n",
    "        for joint_name in self.joint_names:\n",
    "            joint_index = self.joint_indices[joint_name]\n",
    "            joint_state = p.getJointState(self.robotId, joint_index)\n",
    "            joint_positions.append(joint_state[0])\n",
    "            joint_velocities.append(joint_state[1])\n",
    "\n",
    "        # Get base position, orientation and velocities.\n",
    "        base_pos, base_orient_quat = p.getBasePositionAndOrientation(self.robotId)\n",
    "        base_orient_euler = p.getEulerFromQuaternion(base_orient_quat)\n",
    "        base_linear_vel, base_angular_vel = p.getBaseVelocity(self.robotId)\n",
    "        \n",
    "        # Concatenate joint states with base state information.\n",
    "        obs = np.array(\n",
    "            joint_positions + \n",
    "            joint_velocities + \n",
    "            list(base_pos) + \n",
    "            list(base_orient_euler) +\n",
    "            list(base_linear_vel) +\n",
    "            list(base_angular_vel), \n",
    "            dtype=np.float32)\n",
    "        \n",
    "        # Use the new reward function.\n",
    "        reward, reward_components = self._calculate_reward()\n",
    "\n",
    "        # End episode if base falls below a threshold.\n",
    "        done = (\n",
    "            bool(base_pos[2] < 0.5) or \n",
    "            bool(abs(base_orient_euler[0]) > 1) or \n",
    "            bool(abs(base_orient_euler[1]) > 1) or\n",
    "            self.elapsed_time > 20  # 20s timer\n",
    "            )\n",
    "        truncated = False\n",
    "        \n",
    "        return obs, reward, done, truncated, {\"reward_components\": reward_components}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        p.resetSimulation()\n",
    "        p.setGravity(0, 0, -9.81)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.loadURDF(\"plane.urdf\")\n",
    "        self.robotId = p.loadURDF(\"urdf/Exoskeleton.urdf\", self.startPos, self.startOrientation)\n",
    "        \n",
    "        self.elapsed_time = 0  # Timer set to 0\n",
    "\n",
    "        # Reset smoothed action\n",
    "        self.prev_action = np.zeros(len(self.joint_names), dtype=np.float32)  \n",
    "        \n",
    "        # Update joint indices.\n",
    "        self.joint_indices = {}\n",
    "        self.num_joints = p.getNumJoints(self.robotId)\n",
    "        for i in range(self.num_joints):\n",
    "            info = p.getJointInfo(self.robotId, i)\n",
    "            name = info[1].decode('utf-8')\n",
    "            if name in self.joint_names:\n",
    "                self.joint_indices[name] = i\n",
    "        \n",
    "        # Reset joints to default positions.\n",
    "        for joint_name in self.joint_names:\n",
    "            joint_index = self.joint_indices[joint_name]\n",
    "            p.resetJointState(self.robotId, joint_index, self.default_joint_positions[joint_name])\n",
    "        \n",
    "        if self.task == \"stand\":\n",
    "            # Apply a random push in the xy plane to the robot's base.\n",
    "            self.random_push = np.random.uniform(low=-3, high=3, size=2) #+-1.5\n",
    "            p.resetBaseVelocity(self.robotId, linearVelocity=[self.random_push[0], self.random_push[1], 0])\n",
    "        \n",
    "        joint_positions = []\n",
    "        joint_velocities = []\n",
    "        for joint_name in self.joint_names:\n",
    "            joint_index = self.joint_indices[joint_name]\n",
    "            joint_state = p.getJointState(self.robotId, joint_index)\n",
    "            joint_positions.append(joint_state[0])\n",
    "            joint_velocities.append(joint_state[1])\n",
    "        \n",
    "        base_pos, base_orient_quat = p.getBasePositionAndOrientation(self.robotId)\n",
    "        base_orient_euler = p.getEulerFromQuaternion(base_orient_quat)\n",
    "        base_linear_vel, base_angular_vel = p.getBaseVelocity(self.robotId)\n",
    "        \n",
    "        # Concatenate joint states with base state information.\n",
    "        obs = np.array(\n",
    "            joint_positions + \n",
    "            joint_velocities + \n",
    "            list(base_pos) + \n",
    "            list(base_orient_euler) +\n",
    "            list(base_linear_vel) +\n",
    "            list(base_angular_vel), \n",
    "            dtype=np.float32)\n",
    "        \n",
    "        return obs, {}\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        positions = []\n",
    "        velocities = []\n",
    "        torques = []\n",
    "        \n",
    "        for joint_name in self.joint_names:\n",
    "            joint_index = self.joint_indices[joint_name]\n",
    "            joint_state = p.getJointState(self.robotId, joint_index)\n",
    "            \n",
    "            # Store position difference from default\n",
    "            current_pos = joint_state[0]\n",
    "            target_pos = self.default_joint_positions[joint_name]\n",
    "            positions.append(current_pos - target_pos)\n",
    "            \n",
    "            # Store velocity and torque\n",
    "            velocities.append(joint_state[1])\n",
    "            torques.append(joint_state[3])\n",
    "        \n",
    "        base_pos, base_orient_quat = p.getBasePositionAndOrientation(self.robotId)\n",
    "        base_orient_euler = p.getEulerFromQuaternion(base_orient_quat)\n",
    "\n",
    "        time_reward = 1.0\n",
    "        height_reward = np.exp(-50 * (base_pos[2] - 1.215) ** 2)\n",
    "        upright_reward = np.exp(-5 * (base_orient_euler[0]** 2 + base_orient_euler[1]** 2 + base_orient_euler[2]**2))\n",
    "\n",
    "        position_penalty = np.exp(-1 * np.sum(np.square(positions))) - 1.0\n",
    "        velocity_penalty = np.exp(-0.01 * np.sum(np.square(velocities))) - 1.0\n",
    "        torque_penalty = np.exp(-1e-4 * np.sum(np.square(torques))) - 1.0\n",
    "\n",
    "        # Walk-specific rewards\n",
    "        travel_reward = base_pos[0]\n",
    "        drift_penalty = -1 * (base_pos[1] ** 2)\n",
    "\n",
    "        if self.task == \"stand\":\n",
    "            reward_components = {\n",
    "                'time': 1.0 * time_reward,\n",
    "                'height': 1.0 * height_reward,\n",
    "                'upright': 1.0 * upright_reward,\n",
    "                'position_penalty': 0.7 * position_penalty,\n",
    "                'velocity_penalty': 0.3 * velocity_penalty,\n",
    "                'torque_penalty': 0.3 * torque_penalty,\n",
    "            }\n",
    "        \n",
    "        if self.task == \"walk\":\n",
    "            reward_components = {\n",
    "                'time': 1.0 * time_reward,\n",
    "                'height': 1.0 * height_reward,\n",
    "                'upright': 1.0 * upright_reward,\n",
    "                'position_penalty': 0.7 * position_penalty,\n",
    "                'velocity_penalty': 0.3 * velocity_penalty,\n",
    "                'torque_penalty': 0.3 * torque_penalty,\n",
    "                'travel_reward': 0.5 * travel_reward,\n",
    "                'drift_penalty': 1.0 * drift_penalty, \n",
    "            }\n",
    "        \n",
    "        reward = sum(reward_components.values())\n",
    "\n",
    "        return reward, reward_components\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect()\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from collections import defaultdict\n",
    "\n",
    "class RewardComponentsCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_components = None\n",
    "\n",
    "    def _init_callback(self):\n",
    "        self.episode_components = [defaultdict(float) for _ in range(self.training_env.num_envs)]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        for env_idx in range(self.training_env.num_envs):\n",
    "            info = self.locals['infos'][env_idx]\n",
    "            if 'reward_components' in info:\n",
    "                components = info['reward_components']\n",
    "                for key, value in components.items():\n",
    "                    self.episode_components[env_idx][key] += value\n",
    "            if self.locals['dones'][env_idx]:\n",
    "                if self.episode_components[env_idx]:\n",
    "                    for key, value in self.episode_components[env_idx].items():\n",
    "                        self.logger.record(f\"reward_components/{key}\", value)\n",
    "                self.episode_components[env_idx] = defaultdict(float)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf5661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./tensorboard\\test_1\n",
      "----------------------------------\n",
      "| reward_components/  |          |\n",
      "|    height           | 85.6     |\n",
      "|    position_penalty | -65.4    |\n",
      "|    time             | 103      |\n",
      "|    torque_penalty   | -30.9    |\n",
      "|    upright          | 24.3     |\n",
      "|    velocity_penalty | -11.2    |\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 138      |\n",
      "|    ep_rew_mean      | 129      |\n",
      "| time/               |          |\n",
      "|    fps              | 3191     |\n",
      "|    iterations       | 1        |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 49152    |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| reward_components/      |             |\n",
      "|    height               | 38.2        |\n",
      "|    position_penalty     | -71.5       |\n",
      "|    time                 | 112         |\n",
      "|    torque_penalty       | -33.6       |\n",
      "|    upright              | 68.2        |\n",
      "|    velocity_penalty     | -11.9       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 133         |\n",
      "|    ep_rew_mean          | 121         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2985        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006167443 |\n",
      "|    clip_fraction        | 0.0789      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.021       |\n",
      "|    learning_rate        | 0.00104     |\n",
      "|    loss                 | 13.2        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00521    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 89.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| reward_components/      |             |\n",
      "|    height               | 53.6        |\n",
      "|    position_penalty     | -77.7       |\n",
      "|    time                 | 120         |\n",
      "|    torque_penalty       | -35.9       |\n",
      "|    upright              | 35.5        |\n",
      "|    velocity_penalty     | -13.4       |\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 127         |\n",
      "|    ep_rew_mean          | 120         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2931        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001428416 |\n",
      "|    clip_fraction        | 0.000275    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 8.31e-05    |\n",
      "|    loss                 | 58.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00115    |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 138         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gonzalo\\Documents\\Python\\exo-trajectory-control-via-rl\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "model_name=\"test\"\n",
    "\n",
    "def linear_schedule(initial_value: float, final_value: float) -> callable:\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        return final_value + (initial_value - final_value) * progress_remaining\n",
    "    return func\n",
    "\n",
    "initial_lr = 2e-3\n",
    "final_lr = 5e-5\n",
    "lr_schedule = linear_schedule(initial_lr, final_lr)\n",
    "\n",
    "def make_env(render=False, task=\"stand\"):\n",
    "    def _init():\n",
    "        env = ExoskeletonEnv(render=render, task=task)\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "for _ in range(1):\n",
    "    num_envs = 24\n",
    "    env_fns = [make_env(render=False, task=\"stand\") for _ in range(num_envs)]\n",
    "    env = SubprocVecEnv(env_fns)\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", \n",
    "                env, \n",
    "                verbose=1, \n",
    "                device='cpu',\n",
    "                tensorboard_log=\"./tensorboard\",\n",
    "                policy_kwargs = dict(\n",
    "                    net_arch=[256, 256]\n",
    "                    ),\n",
    "                batch_size=8192,\n",
    "                learning_rate=lr_schedule,\n",
    "                gamma=0.995,\n",
    "                )\n",
    "\n",
    "    checkpoint_callback = CheckpointCallback(save_freq=10_000, save_path='./checkpoints/', name_prefix=model_name)\n",
    "    components_callback = RewardComponentsCallback()\n",
    "\n",
    "    model.learn(total_timesteps=10_000_000,\n",
    "                callback=CallbackList([checkpoint_callback, components_callback]),\n",
    "                tb_log_name=model_name \n",
    "                )\n",
    "\n",
    "    model.save(os.path.join(\"models\", model_name))\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import time\n",
    "\n",
    "env = ExoskeletonEnv(render=True, task=\"stand\")\n",
    "\n",
    "model = PPO.load(\"test\", device='cpu')\n",
    "\n",
    "num_episodes = 30\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    print(f\"Starting episode {episode+1}...\")\n",
    "    while not done:\n",
    "        # Use deterministic policy for testing.\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        time.sleep(1./240.)\n",
    "    print(f\"Episode {episode+1} finished with total reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
